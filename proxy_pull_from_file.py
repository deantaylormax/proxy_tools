#gathers proxy from url with free proxies and puts them into a list
#I run this script using a cronjob so that it runs in the background refreshing this list every ten minutes
import time
from datetime import datetime
import requests
from bs4 import BeautifulSoup
import csv
from random import choice
import random

def save_proxy_lst():  #this gets all proxies and corresponding port numbers and puts them into a list 'proxy:port'

    url = 'https://www.us-proxy.org'  #website that stores and updates US proxies
    #url = 'https://free-proxy-list.net/anonymous-proxy.html' #website that stores and updates anonymous proxies, optional site to use with the code
    r = requests.get(url)
    soup = BeautifulSoup(r.content, 'lxml')
    table = soup.find('table')
    rows = table.find_all('tr')
    count = 0
    proxy_lst = []
    for row in rows:
        ip = row.contents[0].text
        port = row.contents[1].text
        anon = row.contents[4].text
        secconn = row.contents[6].text  #this column at the url designates whether the proxy enables secure connection (https) or not
        # if anon == 'elite proxy' or anon == 'anonymous':  #option to pick both types of proxies for your list
        if anon == 'elite proxy':
        #if (secconn == 'yes' and (anon == 'elite proxy' or anon == 'anonymous')):  #other varieties of choices of proxies
        #if (secconn == 'no' and (anon == 'elite proxy' or anon == 'anonymous')):
            print(ip, port, secconn, anon)
            line = 'http://' + ip + ':' + port
            proxies = { 'http': line, 'https': line }
            try:  #this code actually checks if the proxy is currently viable by entering it at httpbin and awaiting a response
                print('Checking if the proxy is good')
                testIP = requests.get('https://httpbin.org/ip', proxies=proxies, timeout=5)
                resIP = testIP.json()['origin']
                origin = resIP.split(',')

                if origin[0] == ip:
                    #print('Good proxy added to list')
                    proxy_lst.append(line)
                    #print(f'len(proxy_lst) good proxies recovered')
                    count += 1
                    if count == 5:  #Set this number to your needs, it will then continue code until this number of proxies are saved to your list
                        break
            except:
                pass
                # print('Bad proxy {}'.format(line))
    #print(proxy_lst)
    data = proxy_lst
    proxy_lst_file = 'proxylst.csv'  #obviously, set this to whatever directory and csv filename you need to save the list of proxies
    with open(proxy_lst_file, 'w', newline='') as csvfile:
        f = csv.writer(csvfile)
        f.writerow(data)

save_proxy_lst()



#pulls random proxy from above created csv list of proxies that is generated by another script
def load_proxy():
    
    myfile = 'proxylst.csv' #use same filename here as you do in the code above to insure you are pulling proxies from created csv with list
    with open(myfile, 'r') as f:
        reader = csv.reader(f)
        your_list = list(reader)
        lst_len = len(your_list[0])
    picknumber = random.choice(list(range(0, lst_len)))  # this creates a random number betwween 0 and the total number of list items
    proxy = your_list[0][picknumber] #picks random proxy
    print(f'proxy chosen is {proxy}')
    return proxy

proxy = load_proxy()  #then use the variable 'proxy' in any code where you want a free proxy inserted, e.g. selenium webdriver options

